{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üå± AI PlantDoc Bot: Intelligent Plant Disease Diagnosis\n",
                "\n",
                "**Project:** AI PlantDoc Bot (Infosys Springboard Virtual Internship)\n",
                "**Domain:** Artificial Intelligence \n",
                "**Author:** SHIVAM SINGH\n",
                "**Date:** 03 December 2025\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ Objective\n",
                "To develop an AI-powered chatbot that allows users (farmers, gardeners) to diagnose plant diseases by uploading leaf images or describing symptoms. The system will utilize **Computer Vision (CNNs)** for image analysis and **NLP (BERT/LLMs)** for symptom interpretation.\n",
                "\n",
                "## üìÖ Day 1 Goals\n",
                "1.  **Environment Setup**: Configure the workspace and dependencies.\n",
                "2.  **Data Acquisition**: Download the **PlantVillage** (Classification) and **PlantDoc** (Object Detection/Noise) datasets.\n",
                "3.  **Data Verification**: Validate directory structures and file integrity.\n",
                "4.  **Exploratory Data Analysis (EDA)**: Analyze class distributions and visualize sample data.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üõ†Ô∏è 1. Environment Setup & Imports\n",
                "# Importing necessary libraries for file handling, visualization, and system operations.\n",
                "\n",
                "import os\n",
                "import glob\n",
                "import shutil\n",
                "import pathlib\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.image as mpimg\n",
                "from dataclasses import dataclass\n",
                "\n",
                "# Using a Configuration class to manage paths and constants.\n",
                "@dataclass\n",
                "class Config:\n",
                "    PROJECT_NAME: str = \"PlantDocBot\"\n",
                "    BASE_DIR: pathlib.Path = pathlib.Path(f\"/content/{PROJECT_NAME}\")\n",
                "    DATA_DIR: pathlib.Path = BASE_DIR / \"data\"\n",
                "    PLANT_VILLAGE_DIR: pathlib.Path = DATA_DIR / \"plantvillage\"\n",
                "    PLANT_DOC_DIR: pathlib.Path = DATA_DIR / \"plantdoc\"\n",
                "    TEXT_CORPUS_DIR: pathlib.Path = DATA_DIR / \"text_corpus\"\n",
                "\n",
                "config = Config()\n",
                "\n",
                "# Ensure base directories exist\n",
                "for directory in [config.PLANT_VILLAGE_DIR, config.PLANT_DOC_DIR, config.TEXT_CORPUS_DIR]:\n",
                "    directory.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f\"‚úÖ Project Structure Created at: {config.BASE_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üì• 2. Data Acquisition\n",
                "# Cloning the required datasets from GitHub. \n",
                "# We check if the data already exists to prevent redundant downloads on re-runs.\n",
                "\n",
                "def clone_repo(repo_url, target_dir):\n",
                "    \"\"\"\n",
                "    Clones a git repository to a target directory if it doesn't already exist.\n",
                "    \"\"\"\n",
                "    if not os.listdir(target_dir):  # Check if directory is empty\n",
                "        print(f\"‚¨áÔ∏è Cloning {repo_url}...\")\n",
                "        !git clone {repo_url} {target_dir}\n",
                "        print(f\"‚úÖ Successfully cloned to {target_dir}\")\n",
                "    else:\n",
                "        print(f\"‚ÑπÔ∏è Data already exists in {target_dir}. Skipping download.\")\n",
                "\n",
                "# PlantVillage Dataset (High-quality classification data)\n",
                "clone_repo(\"https://github.com/spMohanty/plantvillage-Dataset.git\", config.PLANT_VILLAGE_DIR)\n",
                "\n",
                "# PlantDoc Dataset (Real-world noisy data)\n",
                "clone_repo(\"https://github.com/pratikkayal/PlantDoc-Dataset.git\", config.PLANT_DOC_DIR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üîç 3. Data Verification\n",
                "# Verifying the contents of the downloaded datasets.\n",
                "\n",
                "def list_contents(directory, name, limit=5):\n",
                "    \"\"\"Lists the first few items in a directory to verify content.\"\"\"\n",
                "    try:\n",
                "        contents = sorted([p.name for p in directory.iterdir()])\n",
                "        print(f\"\\nüìÇ Contents of {name} ({len(contents)} items):\")\n",
                "        print(f\"   {contents[:limit]} ...\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error reading {name}: {e}\")\n",
                "\n",
                "list_contents(config.PLANT_VILLAGE_DIR, \"PlantVillage\")\n",
                "list_contents(config.PLANT_DOC_DIR, \"PlantDoc\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üìä 4. Exploratory Data Analysis (EDA)\n",
                "# Searching for image directories and calculating statistics.\n",
                "\n",
                "def analyze_dataset(base_path, dataset_name):\n",
                "    \"\"\"\n",
                "    Walks through the directory to find image classes and counts.\n",
                "    \"\"\"\n",
                "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
                "    class_counts = {}\n",
                "    \n",
                "    print(f\"\\nüîé Analyzing {dataset_name}...\")\n",
                "    \n",
                "    for root, dirs, files in os.walk(base_path):\n",
                "        # Count images in current directory\n",
                "        images = [f for f in files if pathlib.Path(f).suffix.lower() in image_extensions]\n",
                "        if images:\n",
                "            class_name = pathlib.Path(root).name\n",
                "            class_counts[class_name] = len(images)\n",
                "            \n",
                "    if not class_counts:\n",
                "        print(f\"‚ö†Ô∏è No image directories found in {dataset_name}. Check the folder structure.\")\n",
                "        return None\n",
                "    \n",
                "    # Sort by count\n",
                "    sorted_counts = sorted(class_counts.items(), key=lambda item: item[1], reverse=True)\n",
                "    \n",
                "    print(f\"‚úÖ Found {len(class_counts)} classes.\")\n",
                "    print(f\"   Total Images: {sum(class_counts.values())}\")\n",
                "    print(f\"   Top 5 Classes by size:\")\n",
                "    for cls, count in sorted_counts[:5]:\n",
                "        print(f\"     - {cls}: {count} images\")\n",
                "        \n",
                "    return sorted_counts\n",
                "\n",
                "# Analyze PlantVillage (Note: The repo structure might be nested, e.g., raw/color)\n",
                "# We search recursively to handle this.\n",
                "pv_stats = analyze_dataset(config.PLANT_VILLAGE_DIR, \"PlantVillage\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üñºÔ∏è 5. Visualization\n",
                "# Displaying a sample image to ensure data integrity.\n",
                "\n",
                "def show_sample_image(base_path):\n",
                "    image_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
                "    all_images = []\n",
                "    \n",
                "    for ext in image_extensions:\n",
                "        all_images.extend(glob.glob(str(base_path / \"**\" / ext), recursive=True))\n",
                "        \n",
                "    if all_images:\n",
                "        sample_img_path = all_images[0]\n",
                "        img = mpimg.imread(sample_img_path)\n",
                "        plt.figure(figsize=(6, 6))\n",
                "        plt.imshow(img)\n",
                "        plt.title(f\"Sample: {pathlib.Path(sample_img_path).parent.name}\")\n",
                "        plt.axis('off')\n",
                "        plt.show()\n",
                "        print(f\"üì∑ Displaying sample from: {sample_img_path}\")\n",
                "    else:\n",
                "        print(\"‚ùå No images found to display.\")\n",
                "\n",
                "show_sample_image(config.PLANT_VILLAGE_DIR)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üìÖ Day 2 Goals: Data Preprocessing & Mapping\n",
                "1.  **Robust Image Visualization**: Ensure images are correctly loaded in RGB format.\n",
                "2.  **Dataset Mapping**: Create a structured CSV file mapping every image path to its label (disease class). This is crucial for training custom models later.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üé® 6. Robust Color Display\n",
                "# Displaying a random image and ensuring it is in RGB format.\n",
                "# This fixes potential issues with RGBA or Grayscale images in the dataset.\n",
                "\n",
                "import random\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "\n",
                "def display_random_image(base_path):\n",
                "    \"\"\"\n",
                "    Selects a random image from the dataset, converts it to RGB, and displays it.\n",
                "    \"\"\"\n",
                "    img_exts = ('.jpg', '.jpeg', '.png', '.bmp')\n",
                "    all_files = []\n",
                "    \n",
                "    # Collect all image files\n",
                "    for root, dirs, files in os.walk(base_path):\n",
                "        for f in files:\n",
                "            if f.lower().endswith(img_exts):\n",
                "                all_files.append(os.path.join(root, f))\n",
                "                \n",
                "    if not all_files:\n",
                "        print(\"‚ùå No images found.\")\n",
                "        return\n",
                "        \n",
                "    # Pick a random file\n",
                "    sample_file = random.choice(all_files)\n",
                "    print(f\"üì∑ Displaying random image: {sample_file}\")\n",
                "    \n",
                "    try:\n",
                "        img = Image.open(sample_file)\n",
                "        print(f\"   Original Mode: {img.mode}\")\n",
                "        \n",
                "        # Convert to RGB if necessary\n",
                "        if img.mode != 'RGB':\n",
                "            img = img.convert('RGB')\n",
                "            print(\"   ‚úÖ Converted to RGB\")\n",
                "            \n",
                "        # Display using Matplotlib\n",
                "        plt.figure(figsize=(6, 6))\n",
                "        plt.imshow(np.asarray(img))\n",
                "        plt.axis('off')\n",
                "        plt.title(f\"Label: {pathlib.Path(sample_file).parent.name}\")\n",
                "        plt.show()\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error opening image: {e}\")\n",
                "\n",
                "# Run the function on PlantVillage data\n",
                "display_random_image(config.PLANT_VILLAGE_DIR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üìù 7. Build CSV Mapping\n",
                "# Creating a CSV file that maps every image path to its corresponding label.\n",
                "# This DataFrame will be the foundation for our PyTorch/TensorFlow data loaders.\n",
                "\n",
                "import pandas as pd\n",
                "\n",
                "def create_image_dataframe(base_path, output_csv_name=\"image_data.csv\"):\n",
                "    \"\"\"\n",
                "    Walks through the directory, infers labels from folder names, and saves to CSV.\n",
                "    \"\"\"\n",
                "    img_exts = ('.jpg', '.jpeg', '.png', '.bmp')\n",
                "    records = []\n",
                "    \n",
                "    print(f\"\\nüìä Building Dataset Mapping for {base_path}...\")\n",
                "    \n",
                "    for root, dirs, files in os.walk(base_path):\n",
                "        for f in files:\n",
                "            if f.lower().endswith(img_exts):\n",
                "                path = os.path.join(root, f)\n",
                "                \n",
                "                # Infer label: The directory name relative to the base path\n",
                "                # Example: data/plantvillage/Tomato_Healthy/001.jpg -> Label: Tomato_Healthy\n",
                "                rel_path = os.path.relpath(path, base_path)\n",
                "                label = rel_path.split(os.sep)[0]\n",
                "                \n",
                "                # Handle nested structures (like 'raw/color/Tomato_Healthy') if necessary\n",
                "                # For now, we assume the immediate parent or the first folder after base is the label\n",
                "                # A more robust way is to use the immediate parent folder name:\n",
                "                label = pathlib.Path(path).parent.name\n",
                "                \n",
                "                records.append({\"image_path\": path, \"label\": label})\n",
                "                \n",
                "    # Create DataFrame\n",
                "    df = pd.DataFrame(records)\n",
                "    \n",
                "    if df.empty:\n",
                "        print(\"‚ö†Ô∏è No images found to map.\")\n",
                "        return None\n",
                "        \n",
                "    # Save to CSV\n",
                "    output_csv_path = config.DATA_DIR / output_csv_name\n",
                "    df.to_csv(output_csv_path, index=False)\n",
                "    \n",
                "    print(f\"‚úÖ Total images mapped: {len(df)}\")\n",
                "    print(f\"‚úÖ Saved mapping to: {output_csv_path}\")\n",
                "    print(\"\\nSample Rows:\")\n",
                "    print(df.head())\n",
                "    \n",
                "    return df\n",
                "\n",
                "# Create the mapping\n",
                "df = create_image_dataframe(config.PLANT_VILLAGE_DIR, \"plantvillage_mapping.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "81486781",
            "metadata": {},
            "source": [
                "---\n",
                "## üìÖ Day 3 Goals: Dataset Expansion & Model Setup\n",
                "1.  **Kaggle Integration**: Download a larger, more diverse dataset (`emmarex/plantdisease`) from Kaggle.\n",
                "2.  **Data Splitting**: Organize the data into `train` and `val` sets (80/20 split) for model training.\n",
                "3.  **Model Architecture**: Initialize a **ResNet50** model using PyTorch, pre-trained on ImageNet, and modify it for our specific disease classes.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f8bf00bd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# üì• 8. Kaggle Dataset Setup\n",
                "# We are switching to a larger dataset from Kaggle for better model performance.\n",
                "# NOTE: You must upload your 'kaggle.json' API key when prompted.\n",
                "\n",
                "import os\n",
                "\n",
                "def setup_kaggle_and_download():\n",
                "    print(\"üîß Setting up Kaggle...\")\n",
                "    # Install Kaggle client\n",
                "    !pip install -q kaggle\n",
                "    \n",
                "    # Handle API Key upload\n",
                "    if not os.path.exists('/root/.kaggle/kaggle.json'):\n",
                "        print(\"‚ö†Ô∏è kaggle.json not found. Please upload it now.\")\n",
                "        from google.colab import files\n",
                "        uploaded = files.upload()\n",
                "        \n",
                "        # Move to correct location\n",
                "        !mkdir -p ~/.kaggle\n",
                "        !cp kaggle.json ~/.kaggle/\n",
                "        !chmod 600 ~/.kaggle/kaggle.json\n",
                "        print(\"‚úÖ Kaggle API key configured.\")\n",
                "    else:\n",
                "        print(\"‚úÖ Kaggle API key already exists.\")\n",
                "        \n",
                "    # Download Dataset\n",
                "    if not os.path.exists(\"/content/plantvillage_data\"):\n",
                "        print(\"‚¨áÔ∏è Downloading emmarex/plantdisease dataset...\")\n",
                "        !kaggle datasets download -d emmarex/plantdisease -p /content\n",
                "        print(\"üì¶ Unzipping dataset...\")\n",
                "        !unzip -q /content/plantdisease.zip -d /content/plantvillage_data\n",
                "        print(\"‚úÖ Download and extraction complete.\")\n",
                "    else:\n",
                "        print(\"‚ÑπÔ∏è Dataset already downloaded.\")\n",
                "\n",
                "setup_kaggle_and_download()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d6a44636",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚úÇÔ∏è 9. Data Splitting (Train/Val)\n",
                "# Organizing the raw images into structured Training and Validation folders.\n",
                "\n",
                "import shutil\n",
                "from sklearn.model_selection import train_test_split\n",
                "from glob import glob\n",
                "\n",
                "def split_dataset():\n",
                "    # Define paths\n",
                "    # Note: The unzipped path structure depends on the zip file content\n",
                "    source_dataset = pathlib.Path(\"/content/plantvillage_data/plantvillage/PlantVillage\")\n",
                "    target_base = config.DATA_DIR \n",
                "    \n",
                "    train_dir = target_base / \"train\"\n",
                "    val_dir = target_base / \"val\"\n",
                "    \n",
                "    # Create directories\n",
                "    train_dir.mkdir(parents=True, exist_ok=True)\n",
                "    val_dir.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    if not source_dataset.exists():\n",
                "        print(f\"‚ùå Source dataset not found at {source_dataset}. Check the unzip step.\")\n",
                "        return\n",
                "\n",
                "    # Detect class folders\n",
                "    class_folders = [d.name for d in source_dataset.iterdir() if d.is_dir()]\n",
                "    print(f\"üîé Detected {len(class_folders)} classes.\")\n",
                "    \n",
                "    for cls in class_folders:\n",
                "        cls_path = source_dataset / cls\n",
                "        \n",
                "        # Get all images (case insensitive)\n",
                "        images = []\n",
                "        for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG']:\n",
                "            images.extend(list(cls_path.glob(ext)))\n",
                "            \n",
                "        if not images:\n",
                "            print(f\"‚ö†Ô∏è No images found in: {cls}\")\n",
                "            continue\n",
                "            \n",
                "        # Split images (80% Train, 20% Val)\n",
                "        train_imgs, val_imgs = train_test_split(images, test_size=0.2, random_state=42)\n",
                "        \n",
                "        # Create class folders in target\n",
                "        (train_dir / cls).mkdir(exist_ok=True)\n",
                "        (val_dir / cls).mkdir(exist_ok=True)\n",
                "        \n",
                "        # Copy files\n",
                "        for img in train_imgs:\n",
                "            shutil.copy(str(img), str(train_dir / cls))\n",
                "            \n",
                "        for img in val_imgs:\n",
                "            shutil.copy(str(img), str(val_dir / cls))\n",
                "            \n",
                "    print(\"‚úÖ Dataset split completed successfully!\")\n",
                "    print(f\"   Train Data: {train_dir}\")\n",
                "    print(f\"   Val Data: {val_dir}\")\n",
                "\n",
                "split_dataset()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3dae26ed",
            "metadata": {},
            "outputs": [],
            "source": [
                "# üß† 10. Model Architecture Setup\n",
                "# Initializing ResNet50 with Transfer Learning.\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torchvision import datasets, models, transforms\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "# Configuration\n",
                "BATCH_SIZE = 32\n",
                "IMG_SIZE = (224, 224)\n",
                "train_dir = config.DATA_DIR / \"train\"\n",
                "val_dir = config.DATA_DIR / \"val\"\n",
                "\n",
                "# Image Transforms (Augmentation for Train, Resize for Val)\n",
                "train_transform = transforms.Compose([\n",
                "    transforms.Resize(IMG_SIZE),\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomRotation(20),\n",
                "    transforms.ToTensor()\n",
                "])\n",
                "\n",
                "val_transform = transforms.Compose([\n",
                "    transforms.Resize(IMG_SIZE),\n",
                "    transforms.ToTensor()\n",
                "])\n",
                "\n",
                "# Load Datasets\n",
                "try:\n",
                "    train_dataset = datasets.ImageFolder(root=str(train_dir), transform=train_transform)\n",
                "    val_dataset = datasets.ImageFolder(root=str(val_dir), transform=val_transform)\n",
                "\n",
                "    # Data Loaders\n",
                "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "    class_names = train_dataset.classes\n",
                "    num_classes = len(class_names)\n",
                "    print(f\"‚úÖ DataLoaders ready. Detected {num_classes} classes: {class_names[:5]}...\")\n",
                "\n",
                "    # Device Configuration\n",
                "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "    print(f\"üöÄ Using device: {device}\")\n",
                "\n",
                "    # Model Initialization (ResNet50)\n",
                "    print(\"üèóÔ∏è Initializing ResNet50 model...\")\n",
                "    model = models.resnet50(weights=\"IMAGENET1K_V2\")\n",
                "\n",
                "    # Modify the final Fully Connected (FC) layer for our number of classes\n",
                "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
                "    model = model.to(device)\n",
                "\n",
                "    # Loss and Optimizer\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
                "    \n",
                "    print(\"‚úÖ Model setup complete. Ready for training.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error setting up model: {e}. Did you run the split_dataset step?\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
